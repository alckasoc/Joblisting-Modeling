{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e721da61",
   "metadata": {},
   "source": [
    "# üß† Predictive Modeling üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56941f44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Table of Contents: <br>\n",
    "    \n",
    "<ul>\n",
    "    <li>1. <a href=\"#1.-Setup\">Setup</a></li>\n",
    "    <li>üìÇ 2. <a href=\"#%F0%9F%93%82-2.-Load-&-Preview-Data\">Load & Preview Data</a></li>\n",
    "    <li>üîß 3. <a href=\"#%F0%9F%94%A7-3.-Preprocessing\">Preprocessing</a>\n",
    "        <ul>\n",
    "            <li>3.1. <a href=\"#3.1.-Splitting-the-Data\">Splitting the Data</a></li>\n",
    "            <li>3.2. <a href=\"#3.2.-Pipeline\">Pipeline</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>üò∂‚Äçüå´Ô∏è 4. <a href=\"#%F0%9F%98%B6%E2%80%8D%F0%9F%8C%AB%EF%B8%8F-4.-Baseline\">Baseline</a></li>\n",
    "    <li>ü§î 5. <a href=\"#%F0%9F%A4%94-5.-Model-Selection\">Model Selection</a></li>\n",
    "    <li>üöß 6. <a href=\"#%F0%9F%9A%A7-6.-Feature-Engineering\">Feature Engineering</a></li>\n",
    "    <li>üß™ 7. <a href=\"#%F0%9F%A7%AA-7.-Hyperparameter-Sweeping\">Hyperparameter Sweeping</a></li>\n",
    "    <li>üî¨ 8. <a href=\"#%F0%9F%94%AC-8.-Ensembling\">Ensembling</a></li>\n",
    "    <li>9. <a href=\"#9.-Conclusion\">Conclusion</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25547546",
   "metadata": {},
   "source": [
    "<h3><center>üõë <b>Disclaimer</b> üõë</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48276292",
   "metadata": {},
   "source": [
    "From my understanding, predictive modeling is more than just taking whatever data is available and passing it through a random model. I'm still learning lots about this! Here's what I know:\n",
    "\n",
    "* it all goes back to the data and the quality of data; models are limited by their data\n",
    "* there is no free lunch; no one model works perfectly\n",
    "* generally larger datasets improve performance (at least in DL)\n",
    "* rather than optimize for performance, business context and use case and other real-world trade-offs have to be considered\n",
    "    * is this model interpretable?\n",
    "    * how fast is it?\n",
    "    * how well does it integrate into the existing infrastructure?\n",
    "    * does this model achieve comparable performance?\n",
    "    * is it ethical? where was the data sourced? is it ok to use this data?\n",
    "    * are there unintended effects?\n",
    "* are there ways to solve/tackle this problem *without* machine learning?\n",
    "* any hardware or financial or other limitations?\n",
    "\n",
    "The list goes on, but generally these considerations fall into data, model, context, or deployment problems (from what I understand)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252508b",
   "metadata": {},
   "source": [
    "For this particular dataset, we face the following problems:\n",
    "* insufficient sample size\n",
    "* not enough predictive features\n",
    "* a few imbalanced classes (full-time vs internships, etc)\n",
    "* data based largely in CA\n",
    "* some roles aren't data scientist roles\n",
    "* revenue has large amount of unknown\n",
    "* (my guess) the current features are not predictive enough for salary estimate\n",
    "* categorical columns have very rare unique values\n",
    "\n",
    "**Note**: Though I don't believe this dataset to be suitable for modeling, I will still treat this like a model-able task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e00c8",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "50f755ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports.\n",
    "import os\n",
    "\n",
    "# Specific imports.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adfdb3b",
   "metadata": {},
   "source": [
    "## üìÇ 2. Load & Preview Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2f806",
   "metadata": {},
   "source": [
    "Let's load in our data and check if there are any immediate problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7b5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/joblisting_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6c4906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>rating</th>\n",
       "      <th>headquarters</th>\n",
       "      <th>salary estimate</th>\n",
       "      <th>job type</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "      <th>sector</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indeed</td>\n",
       "      <td>4.3</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>209.0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>10000+ employees</td>\n",
       "      <td>company - private</td>\n",
       "      <td>information technology</td>\n",
       "      <td>$2 to $5 billion (usd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indeed</td>\n",
       "      <td>4.3</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>143.0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>10000+ employees</td>\n",
       "      <td>company - private</td>\n",
       "      <td>information technology</td>\n",
       "      <td>$2 to $5 billion (usd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abl Schools</td>\n",
       "      <td>4.1</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>1 to 50 employees</td>\n",
       "      <td>company - private</td>\n",
       "      <td>business services</td>\n",
       "      <td>unknown / non-applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>115.0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>10000+ employees</td>\n",
       "      <td>company - public</td>\n",
       "      <td>information technology</td>\n",
       "      <td>$10+ billion (usd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thermo Fisher - America</td>\n",
       "      <td>3.8</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>134.5</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>10000+ employees</td>\n",
       "      <td>company - public</td>\n",
       "      <td>biotech &amp; pharmaceuticals</td>\n",
       "      <td>$10+ billion (usd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>Kaiser Permanente</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Oakland, CA</td>\n",
       "      <td>101.5</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>10000+ employees</td>\n",
       "      <td>nonprofit organization</td>\n",
       "      <td>health care</td>\n",
       "      <td>$10+ billion (usd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>California State University</td>\n",
       "      <td>4.3</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>81.0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>201 to 500 employees</td>\n",
       "      <td>college / university</td>\n",
       "      <td>education</td>\n",
       "      <td>$1 to $5 million (usd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>Adobe</td>\n",
       "      <td>4.4</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>151.5</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>10000+ employees</td>\n",
       "      <td>company - public</td>\n",
       "      <td>information technology</td>\n",
       "      <td>$5 to $10 billion (usd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>First Republic Bank</td>\n",
       "      <td>4.3</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>112.5</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>company - public</td>\n",
       "      <td>finance</td>\n",
       "      <td>$10+ billion (usd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>Agama Solutions</td>\n",
       "      <td>3.7</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>134.0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>51 to 200 employees</td>\n",
       "      <td>contract</td>\n",
       "      <td>information technology</td>\n",
       "      <td>$10 to $25 million (usd)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1701 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          company  rating       headquarters  salary estimate  \\\n",
       "0                          Indeed     4.3  San Francisco, CA            209.0   \n",
       "1                          Indeed     4.3  San Francisco, CA            143.0   \n",
       "2                     Abl Schools     4.1  San Francisco, CA            140.0   \n",
       "3                          Amazon     3.8      Palo Alto, CA            115.0   \n",
       "4         Thermo Fisher - America     3.8  San Francisco, CA            134.5   \n",
       "...                           ...     ...                ...              ...   \n",
       "1696            Kaiser Permanente     4.0        Oakland, CA            101.5   \n",
       "1697  California State University     4.3  San Francisco, CA             81.0   \n",
       "1698                        Adobe     4.4  San Francisco, CA            151.5   \n",
       "1699          First Republic Bank     4.3  San Francisco, CA            112.5   \n",
       "1700              Agama Solutions     3.7  San Francisco, CA            134.0   \n",
       "\n",
       "       job type                    size                    type  \\\n",
       "0     Full-time        10000+ employees       company - private   \n",
       "1     Full-time        10000+ employees       company - private   \n",
       "2     Full-time       1 to 50 employees       company - private   \n",
       "3     Full-time        10000+ employees        company - public   \n",
       "4     Full-time        10000+ employees        company - public   \n",
       "...         ...                     ...                     ...   \n",
       "1696  Full-time        10000+ employees  nonprofit organization   \n",
       "1697  Full-time    201 to 500 employees    college / university   \n",
       "1698  Full-time        10000+ employees        company - public   \n",
       "1699  Full-time  1001 to 5000 employees        company - public   \n",
       "1700  Full-time     51 to 200 employees                contract   \n",
       "\n",
       "                         sector                   revenue  \n",
       "0        information technology    $2 to $5 billion (usd)  \n",
       "1        information technology    $2 to $5 billion (usd)  \n",
       "2             business services  unknown / non-applicable  \n",
       "3        information technology        $10+ billion (usd)  \n",
       "4     biotech & pharmaceuticals        $10+ billion (usd)  \n",
       "...                         ...                       ...  \n",
       "1696                health care        $10+ billion (usd)  \n",
       "1697                  education    $1 to $5 million (usd)  \n",
       "1698     information technology   $5 to $10 billion (usd)  \n",
       "1699                    finance        $10+ billion (usd)  \n",
       "1700     information technology  $10 to $25 million (usd)  \n",
       "\n",
       "[1701 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d68443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1701, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d2478f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['company', 'rating', 'headquarters', 'salary estimate', 'job type',\n",
       "       'size', 'type', 'sector', 'revenue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d8336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1701 entries, 0 to 1700\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   company          1701 non-null   object \n",
      " 1   rating           1701 non-null   float64\n",
      " 2   headquarters     1701 non-null   object \n",
      " 3   salary estimate  1701 non-null   float64\n",
      " 4   job type         1701 non-null   object \n",
      " 5   size             1701 non-null   object \n",
      " 6   type             1701 non-null   object \n",
      " 7   sector           1701 non-null   object \n",
      " 8   revenue          1701 non-null   object \n",
      "dtypes: float64(2), object(7)\n",
      "memory usage: 119.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d06287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.3, 4.1, 3.8, 3.4, 4.8, 4.6, 4. , 3.2, 4.5, 4.2, 4.7, 4.4, 4.9,\n",
       "       5. , 3.7, 3.5, 3.6, 3.9, 2.6, 2.8, 3.3, 3.1, 3. , 2.9, 2.5, 2.3,\n",
       "       2.7, 2.1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333b5332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['San Francisco, CA', 'Palo Alto, CA', 'Menlo Park, CA',\n",
       "       'Sunnyvale, CA', 'Mountain View, CA', 'Newark, CA', 'Oakland, CA',\n",
       "       'Redwood City, CA', 'Cupertino, CA', 'Burlingame, CA',\n",
       "       'Emeryville, CA', 'South San Francisco, CA', 'San Jose, CA',\n",
       "       'San Mateo, CA', 'Santa Clara, CA', 'Los Gatos, CA',\n",
       "       'Sausalito, CA', 'Brisbane, CA', 'Elk Grove, CA',\n",
       "       'Foster City, CA', 'Pittsburg, CA', 'Sacramento, CA',\n",
       "       'Fremont, CA', 'Concord, CA', 'East Palo Alto, CA',\n",
       "       'Patterson, CA', 'San Bruno, CA', 'Pleasanton, CA', 'Hercules, CA',\n",
       "       'Berkeley, CA', 'Morgan Hill, CA', 'San Ramon, CA', 'Milpitas, CA',\n",
       "       'San Carlos, CA', 'Alameda, CA', 'Scotts Valley, CA',\n",
       "       'Livermore, CA', 'Petaluma, CA', 'Greenbrae, CA',\n",
       "       'Rancho Cordova, CA', 'Saint Helena, CA', 'Oakdale, CA',\n",
       "       'Clearlake, CA', 'Walnut Creek, CA', 'Lodi, CA', 'Dublin, CA',\n",
       "       'Los Altos, CA', 'Stanford, CA', 'Richmond, CA', 'Hayward, CA',\n",
       "       'Martinez, CA', 'Davis, CA', 'West Sacramento, CA', 'Novato, CA'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.headquarters.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c46c15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.5, 209.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"salary estimate\"].min(), df[\"salary estimate\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6132038d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Full-time', 'Part-time', 'Contract', 'Internship'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "577e2506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10000+ employees', '1 to 50 employees', '201 to 500 employees',\n",
       "       '51 to 200 employees', '1001 to 5000 employees',\n",
       "       '501 to 1000 employees', '5001 to 10000 employees'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"size\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99831f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['company - private', 'company - public', 'nonprofit organization',\n",
       "       'subsidiary or business segment', 'government',\n",
       "       'college / university', 'self-employed', 'contract', 'hospital',\n",
       "       'school / school district'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45dca263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['information technology', 'business services',\n",
       "       'biotech & pharmaceuticals', 'finance', 'retail', 'insurance',\n",
       "       'manufacturing', 'non-profit', 'health care',\n",
       "       'oil, gas, energy & utilities', 'travel & tourism', 'government',\n",
       "       'media', 'accounting & legal', 'arts, entertainment & recreation',\n",
       "       'real estate', 'telecommunications', 'transportation & logistics',\n",
       "       'education', 'construction, repair & maintenance',\n",
       "       'agriculture & forestry', 'restaurants, bars & food services'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sector\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29dea795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['$2 to $5 billion (usd)', 'unknown / non-applicable',\n",
       "       '$10+ billion (usd)', '$25 to $50 million (usd)',\n",
       "       '$10 to $25 million (usd)', 'less than $1 million (usd)',\n",
       "       '$100 to $500 million (usd)', '$5 to $10 million (usd)',\n",
       "       '$500 million to $1 billion (usd)', '$1 to $5 million (usd)',\n",
       "       '$50 to $100 million (usd)', '$1 to $2 billion (usd)',\n",
       "       '$5 to $10 billion (usd)'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"revenue\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f961cf",
   "metadata": {},
   "source": [
    "Alright, from an immediate inspection, it looks like we don't have any NaNs. This is good! Our cleaning pipeline works. The only nuance I can see is the \"unknown / non-applicable\" category for the \"revenue\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a6374",
   "metadata": {},
   "source": [
    "## üîß 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06cc44",
   "metadata": {},
   "source": [
    "Let's set up a preprocessing pipeline. This data puts us in a precarious position. We preprocess as usual except the numerous categorical features have very rare unique values. How do we combat this?\n",
    "* we can `fit` the pipeline to the entire dataset before actually transforming any data (whether that data be test, val, or train)\n",
    "    * however, this has the problem of leveraging data your training pipeline *shouldn't* be seeing\n",
    "* for unique categorical values that occur less than a specified threshold, we can convert it to \"other\"\n",
    "    * basically, we fit the pipeline to training data, any rare occurrences in the train data below some threshold is converted to an \"other\" class\n",
    "    * though do note that during test time, we would just be grouping any unfamiliar unique categorical value for a feature into \"other\"\n",
    "    \n",
    "Let's go with the 2nd method. It is more generalizable to test data. Though, of course, by grouping rare occurrences together, we are losing out on a good chunk of data. :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3582930",
   "metadata": {},
   "source": [
    "### 3.1. Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78d8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"salary estimate\"]\n",
    "X = df.drop(columns=[\"salary estimate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d05b7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72a9db14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1377, 8) (153, 8) (171, 8) (1377,) (153,) (171,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33511f35",
   "metadata": {},
   "source": [
    "### 3.2. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc1b8681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['company', 'rating', 'headquarters', 'salary estimate', 'job type',\n",
       "       'size', 'type', 'sector', 'revenue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b396c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_attribs = [\"company\"]\n",
    "num_attribs = [\"rating\"]\n",
    "cat_attribs = [\"headquarters\", \"job type\", \"size\", \"type\", \"sector\", \"revenue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5dd84eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"one_hot\", OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23e7ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRemover(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, useless_attribs):\n",
    "        self.useless_attribs = useless_attribs\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy = X_copy.drop(self.useless_attribs, axis=1)\n",
    "        return X_copy\n",
    "    \n",
    "class CatGrouper(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols, threshold=0.1):\n",
    "        self.cols = cols\n",
    "        self.threshold = threshold\n",
    "        self.cat_groups = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        for attrib in self.cols:\n",
    "            vc = df[attrib].value_counts(normalize=True)\n",
    "            thres = vc < self.threshold\n",
    "            keep = vc[np.logical_not(thres)].index\n",
    "            self.cat_groups[attrib] = list(keep)\n",
    "        return self\n",
    "    \n",
    "    def _map_func(self, v, attrib):\n",
    "        if v not in self.cat_groups[attrib]:\n",
    "            v = \"Other\"\n",
    "        return v\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for attrib in self.cols:\n",
    "            X_copy[attrib] = X_copy[attrib].apply(self._map_func, attrib=attrib)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bb4ac6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs)\n",
    "])\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    (\"remover\", CustomRemover(remove_attribs)),\n",
    "    (\"cat_grouper\", CatGrouper(cat_attribs, threshold=0.05)),\n",
    "    (\"base\", base_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e84c4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_val_prep = preprocessing_pipeline.transform(X_val)\n",
    "X_test_prep = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139a493",
   "metadata": {},
   "source": [
    "Alright, we just finished with the preprocessing! It was fairly straightforward except I had to include a custom transformer for the rare categorical values. Let's start on our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4548f",
   "metadata": {},
   "source": [
    "## üò∂‚Äçüå´Ô∏è 4. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61fbee",
   "metadata": {},
   "source": [
    "Before we dive straight into more complex, black-box models and before we develop more complex systems, let's try a simple baseline. This method is helpful for gauging how usable a dataset is and how complex of a model you need to tackle this dataset. Since we are aiming to predict salary estimate, this is a regression task. Let's start with a simple Linear Regression model. This model is fast but simple. It will probably be beaten by stronger methods.\n",
    "\n",
    "And also, note, before or after preprocessing, dimensionality reduction should be considered especially if a lot of the data is not relevant. However, considering how small our dataset is, we won't DR the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "021123b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1377x30 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 9639 stored elements in Compressed Sparse Row format>,\n",
       " 66      165.0\n",
       " 1247    160.0\n",
       " 892     144.0\n",
       " 313     155.0\n",
       " 689     142.0\n",
       "         ...  \n",
       " 653     139.0\n",
       " 1462    151.0\n",
       " 223     124.5\n",
       " 1674     88.5\n",
       " 1115    105.5\n",
       " Name: salary estimate, Length: 1377, dtype: float64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "743d415f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train_prep, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e451d",
   "metadata": {},
   "source": [
    "That was quick! We have a small dataset and this model is relatively simple. I just showed how to *train* a single model. Let's train an ensemble of linear regression models and cross-validate them to see how linear regression *really* performs on this dataset.\n",
    "\n",
    "There are a variety of metrics to track. The 2 most popular regression metrics are **mean squared error (MSE)** and **mean absolute error (MAE)**. Choosing a metric to evaluate your model on is crucial. Here we can afford to test multiple metrics to get a sense of which one is most useful, but in certain cases, switching between metrics is costly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "009ddc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "n_jobs = -1\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "52203585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-0: -20.093198584264933\n",
      "Fold-1: -26.169084126622074\n",
      "Fold-2: -21.43110252124096\n",
      "Fold-3: -18.473662959962766\n",
      "Fold-4: -20.3323498451929\n",
      "\n",
      "Mean Score: -21.299879607456727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(reg, X_val_prep, y_val, \n",
    "                         cv=n_folds, scoring=\"neg_mean_absolute_error\", \n",
    "                         n_jobs=n_jobs, verbose=verbose)\n",
    "\n",
    "for fold, score in zip(range(n_folds), scores):\n",
    "    print(f\"Fold-{fold}: {score}\")\n",
    "    \n",
    "print(f\"\\nMean Score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "84d66228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-0: -645.5809525966392\n",
      "Fold-1: -935.6621520610457\n",
      "Fold-2: -687.1457157617054\n",
      "Fold-3: -498.10166476597925\n",
      "Fold-4: -661.864560345873\n",
      "\n",
      "Mean Score: -685.6710091062486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(reg, X_val_prep, y_val, \n",
    "                         cv=n_folds, scoring=\"neg_mean_squared_error\", \n",
    "                         n_jobs=n_jobs, verbose=verbose)\n",
    "\n",
    "for fold, score in zip(range(n_folds), scores):\n",
    "    print(f\"Fold-{fold}: {score}\")\n",
    "    \n",
    "print(f\"\\nMean Score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7edb45",
   "metadata": {},
   "source": [
    "Look at the size of those numbers for MSE! They are insanely large. I think it's safe to say MAE would be better in this context for judging model performance. This is because, from our EDA, we noticed that there were outliers in the salary estimates. From what I recall, one joblisting had an estimated salary of 24.5k compared to the dataset mean of ~135k.\n",
    "\n",
    "Let's test a few other regressors in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2754c",
   "metadata": {},
   "source": [
    "## ü§î 5. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb8bbb",
   "metadata": {},
   "source": [
    "This section will describe a few more advanced regressors. We will also test them before doing a hyperparameter sweep. We want to narrow down to get the best models before sweeping and fine-tuning them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b780025",
   "metadata": {},
   "source": [
    "The models we will consider for this task are:\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Extra Trees\n",
    "* SVM\n",
    "* XGBoost\n",
    "* Catboost\n",
    "* LightGBM\n",
    "* dense shallow neural network\n",
    "\n",
    "Model selection is a tricky business. I didn't include common techniques like ElasticNet or Ridge or LASSO because they are similar to linear regression in that they add regularization constraint components. Here we consider speed versus accuracy. Decision Tree and SVM (because of our small dataset) are both fast. The models are a bit slower. However, the slower models make up for it by generally performing better. What's nice is that some of these implementations (i.e. LightGBM) are optimized for efficiency and speed. I predict that either XGBoost or LightGBM wins in this model selection competition!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fbf522",
   "metadata": {},
   "source": [
    "## üöß 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23cd073",
   "metadata": {},
   "source": [
    "## üß™ 7. Hyperparameter Sweeping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095ad3b",
   "metadata": {},
   "source": [
    "## üî¨ 8. Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afb9e9",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b9567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
